/**
 * LLM-as-Judge Evals for Compiler Quality
 *
 * Measures how well the compiler synthesizes coherent skills
 * from functional compositions.
 */
import Anthropic from '@anthropic-ai/sdk';
import { skill, pipe, parallel, fork, hydrate } from '../src/index.js';
import { compileWithAgent } from '../src/compiler/index.js';
import type { CompositionNode } from '../src/core/types.js';

interface EvalCase {
  name: string;
  description: string;
  composition: CompositionNode;
  minScores: {
    coherence: number;
    flow: number;
    completeness: number;
  };
}

interface EvalResult {
  name: string;
  passed: boolean;
  scores: {
    coherence: number;
    flow: number;
    completeness: number;
    reasoning: string;
  };
  compiledContent: string;
}

const JUDGE_PROMPT = `You are evaluating a compiled skill document. The skill was generated by combining multiple smaller skills using functional composition (pipe, parallel, fork, hydrate).

Score the output on these criteria (1-5 scale):

1. **Coherence** (1-5): Does it read as a unified document, not pasted-together sections?
   - 5: Completely unified, reads like it was written as one piece
   - 3: Some transitions feel mechanical
   - 1: Clearly separate sections pasted together

2. **Flow** (1-5): Does the sequence feel natural and logical?
   - 5: Perfect logical progression
   - 3: Reasonable but some awkward transitions
   - 1: Jumps around without clear logic

3. **Completeness** (1-5): Is all information from the source skills preserved?
   - 5: All details preserved, nothing lost
   - 3: Most information present, minor omissions
   - 1: Significant information missing

Return ONLY valid JSON in this exact format:
{"coherence": N, "flow": N, "completeness": N, "reasoning": "Brief explanation"}`;

// Define test cases
const evalCases: EvalCase[] = [
  {
    name: 'simple-pipe',
    description: 'Two skills in sequence',
    composition: pipe(
      skill({ name: 'gather', instructions: 'Gather relevant data from the source system.' }),
      skill({ name: 'analyze', instructions: 'Analyze the gathered data for patterns.' })
    ),
    minScores: { coherence: 3, flow: 3, completeness: 4 },
  },
  {
    name: 'parallel-gather',
    description: 'Parallel data gathering followed by analysis',
    composition: pipe(
      parallel(
        skill({ name: 'search-logs', instructions: 'Search application logs for errors.' }),
        skill({ name: 'search-metrics', instructions: 'Check system metrics for anomalies.' })
      ),
      skill({ name: 'correlate', instructions: 'Correlate findings from both sources.' })
    ),
    minScores: { coherence: 3, flow: 3, completeness: 4 },
  },
  {
    name: 'conditional-branch',
    description: 'Fork based on condition',
    composition: pipe(
      skill({ name: 'assess', instructions: 'Assess the severity of the situation.' }),
      fork({
        when: 'severity === "critical"',
        then: skill({ name: 'escalate', instructions: 'Escalate to on-call immediately.' }),
        else: skill({ name: 'log', instructions: 'Log for later review.' }),
      })
    ),
    minScores: { coherence: 3, flow: 3, completeness: 4 },
  },
  {
    name: 'hydrated-skill',
    description: 'Skill with injected configuration',
    composition: hydrate(
      skill({
        name: 'search',
        instructions: 'Search the configured data source for relevant information.',
      }),
      { dataSource: 'production-db', timeout: '30s', maxResults: 100 }
    ),
    minScores: { coherence: 4, flow: 4, completeness: 4 },
  },
  {
    name: 'complex-workflow',
    description: 'Complex composition with all operators',
    composition: pipe(
      parallel(
        hydrate(
          skill({ name: 'fetch-logs', instructions: 'Fetch logs from the logging system.' }),
          { service: 'api-gateway', timeRange: '1h' }
        ),
        hydrate(
          skill({ name: 'fetch-metrics', instructions: 'Fetch metrics from monitoring.' }),
          { dashboard: 'api-health', period: '1h' }
        )
      ),
      skill({ name: 'analyze', instructions: 'Analyze combined data for root cause.' }),
      fork({
        when: 'confidence > 0.8',
        then: skill({ name: 'auto-remediate', instructions: 'Apply automatic fix.' }),
        else: skill({ name: 'alert-human', instructions: 'Alert engineer for review.' }),
      })
    ),
    minScores: { coherence: 3, flow: 3, completeness: 3 },
  },
];

async function runJudge(content: string): Promise<{
  coherence: number;
  flow: number;
  completeness: number;
  reasoning: string;
}> {
  const client = new Anthropic();

  const response = await client.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 500,
    messages: [
      {
        role: 'user',
        content: `${JUDGE_PROMPT}\n\n---\n\nCompiled skill to evaluate:\n\n${content}`,
      },
    ],
  });

  const text = response.content
    .filter((b) => b.type === 'text')
    .map((b) => b.text)
    .join('');

  try {
    return JSON.parse(text);
  } catch {
    console.error('Failed to parse judge response:', text);
    return { coherence: 0, flow: 0, completeness: 0, reasoning: 'Parse error' };
  }
}

async function runEval(evalCase: EvalCase): Promise<EvalResult> {
  console.log(`  Running: ${evalCase.name}...`);

  // Compile the composition
  const compiled = await compileWithAgent(evalCase.composition, {
    name: evalCase.name,
    description: evalCase.description,
  });

  // Judge the output
  const scores = await runJudge(compiled.content);

  // Check if passed
  const passed =
    scores.coherence >= evalCase.minScores.coherence &&
    scores.flow >= evalCase.minScores.flow &&
    scores.completeness >= evalCase.minScores.completeness;

  return {
    name: evalCase.name,
    passed,
    scores,
    compiledContent: compiled.content,
  };
}

async function main() {
  console.log('ðŸ§ª Running LLM-as-Judge Evals\n');
  console.log(`ðŸ“‹ ${evalCases.length} test cases\n`);

  const results: EvalResult[] = [];

  for (const evalCase of evalCases) {
    const result = await runEval(evalCase);
    results.push(result);

    const status = result.passed ? 'âœ…' : 'âŒ';
    console.log(
      `  ${status} ${result.name}: coherence=${result.scores.coherence}, flow=${result.scores.flow}, completeness=${result.scores.completeness}`
    );
  }

  console.log('\n--- Summary ---\n');

  const passed = results.filter((r) => r.passed).length;
  const failed = results.filter((r) => !r.passed).length;

  console.log(`âœ… Passed: ${passed}/${results.length}`);
  console.log(`âŒ Failed: ${failed}/${results.length}`);

  if (failed > 0) {
    console.log('\nFailed cases:');
    for (const result of results.filter((r) => !r.passed)) {
      console.log(`  - ${result.name}: ${result.scores.reasoning}`);
    }
  }

  // Calculate averages
  const avgCoherence = results.reduce((sum, r) => sum + r.scores.coherence, 0) / results.length;
  const avgFlow = results.reduce((sum, r) => sum + r.scores.flow, 0) / results.length;
  const avgCompleteness = results.reduce((sum, r) => sum + r.scores.completeness, 0) / results.length;

  console.log('\nðŸ“Š Average Scores:');
  console.log(`   Coherence: ${avgCoherence.toFixed(2)}/5`);
  console.log(`   Flow: ${avgFlow.toFixed(2)}/5`);
  console.log(`   Completeness: ${avgCompleteness.toFixed(2)}/5`);

  // Exit with error if any failed
  if (failed > 0) {
    process.exit(1);
  }
}

main().catch(console.error);
